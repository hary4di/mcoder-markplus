# M-Code Pro - Scalability & Modernization Plan
**Date**: January 8, 2026  
**Target**: Support 20+ concurrent users, Modern UI/UX, Multi-source data support

---

## ðŸŽ¯ **PROBLEMS IDENTIFIED**

### 1. **Scalability Issues** ðŸ”´ CRITICAL
**Current State**:
- Gunicorn: 1 worker only (in-memory progress tracking)
- 12 concurrent users = crashes, errors, stuck processes
- Classification blocks request thread (user must keep browser open)

**Impact**:
- Cannot scale beyond 5-10 users
- Poor user experience (macet, error)
- Production instability

### 2. **UI/UX Issues** ðŸŸ¡ HIGH
**Current State**:
- Menu redundancy: "Start Classification" menu not needed
- Navigation: 3 clicks to start classification (Dashboard â†’ Classify â†’ Upload)
- Not intuitive for new users
- No visual feedback for background tasks

**Impact**:
- User confusion
- Inefficient workflow
- Hard to expand (tabulation, non-Kobo sources)

### 3. **502 Bad Gateway** ðŸ”´ CRITICAL
**Current State**:
- Nginx timeout (likely 60s default)
- Gunicorn timeout: 300s but conflicts with Nginx
- View result page crashes after classification

**Impact**:
- Users cannot see results
- Data loss perception
- Production unusable

---

## ðŸš€ **SOLUTION ARCHITECTURE**

### **Phase 1: Immediate Fixes** (Week 1 - Jan 8-12, 2026)
**Goal**: Make production stable for 20+ users

#### 1.1 Implement Celery + Redis
**Why**: Proper background task queue, industry standard
- **Redis**: Message broker, progress tracking, session storage
- **Celery**: Distributed task queue (async classification)
- **Benefit**: Classification runs independent of HTTP request

**Technical Changes**:
```
app/
â”œâ”€â”€ celery_app.py          # NEW - Celery initialization
â”œâ”€â”€ tasks/                 # NEW - Celery tasks
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classification.py  # Classification as Celery task
â”‚   â””â”€â”€ progress.py        # Progress tracking with Redis
â”œâ”€â”€ models.py              # Add task_id field to ClassificationJob
â””â”€â”€ routes.py              # Submit classification â†’ Celery task
```

**Infrastructure**:
- Install Redis on VPS
- Configure Celery workers (4-8 workers recommended)
- Update Gunicorn to 4+ workers (no longer limited by in-memory)

#### 1.2 Fix Nginx Timeout
```nginx
# /etc/nginx/sites-available/mcoder
location / {
    proxy_connect_timeout 600s;
    proxy_send_timeout 600s;
    proxy_read_timeout 600s;
    send_timeout 600s;
}
```

#### 1.3 Database Connection Pool
```python
# config.py
SQLALCHEMY_ENGINE_OPTIONS = {
    'pool_size': 20,        # Up from default 5
    'max_overflow': 40,     # Handle burst traffic
    'pool_pre_ping': True,  # Check connection before use
    'pool_recycle': 3600    # Recycle after 1 hour
}
```

**Expected Outcome**:
- âœ… Support 20+ concurrent users
- âœ… Classification continues if user closes browser
- âœ… No more 502 errors
- âœ… Stable production environment

**Time Estimate**: 2-3 days
**Deployment**: Requires VPS maintenance window (2-3 hours downtime)

---

### **Phase 2: UI/UX Redesign** (Week 2 - Jan 13-19, 2026)
**Goal**: Simplify navigation, modern interface, intuitive workflow

#### 2.1 Navigation Redesign
**Current**:
```
Dashboard â†’ Start Classification â†’ Upload Files â†’ Select Variables â†’ Run
                (redundant)
```

**Proposed**:
```
Dashboard (with quick action cards)
â”œâ”€ Upload & Classify  â† Direct action
â”œâ”€ Results History    â† View past jobs
â”œâ”€ Analytics          â† Super admin only
â””â”€ Settings
```

**Dashboard Cards** (Hero Section):
1. **Upload New Dataset** - Primary CTA, large card with icon
2. **Recent Classifications** - 5 latest jobs with status badges
3. **Quick Stats** - Files processed, variables, success rate
4. **Tabulation Module** - Coming Soon badge (Phase 3)

#### 2.2 Unified Upload Interface
**Design**:
- Single-page workflow with progress steps
- Drag & drop file upload (not just file picker)
- Live preview of detected variables
- One-click "Start Classification" button
- No separate "Start Classification" menu

**Steps Visualization**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Upload    Step 2: Select      â”‚
â”‚     [â—]                [â—‹]              â”‚
â”‚      â†“                  â†“               â”‚
â”‚  Drop files here   â†’ Select variables   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.3 Real-Time Progress
**Features**:
- Live progress bar with percentage
- Current step indicator (e.g., "Generating categories: 45%")
- Estimated time remaining
- Cancel button (stop Celery task)
- Toast notifications when complete

#### 2.4 Results Page Enhancement
**New Features**:
- **Data Source Badge**: Kobo / Excel / CSV (for future multi-source)
- **Processing Status**: Success / Partial / Failed with color coding
- **Quick Actions**: Re-run, Download, Share, Archive
- **Visual Category Distribution**: Chart.js pie/bar charts
- **Confidence Score Overview**: Average + distribution

**Mockup**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ“Š Classification Results                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ [Kobo] Dataset: ASDP_Berkendara.xlsx    â”‚
â”‚ â±ï¸ Processed: 2m 34s | âœ“ 142 responses  â”‚
â”‚                                          â”‚
â”‚ Variables Classified:                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ E1: Evaluasi Produk                 â”‚ â”‚
â”‚ â”‚ ðŸ·ï¸ 8 categories | ðŸ“ˆ 92% confidence â”‚ â”‚
â”‚ â”‚ [View Details] [Download CSV]       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                          â”‚
â”‚ [ðŸ“¥ Download All] [ðŸ”„ Re-run] [ðŸ“¤ Share]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.5 Mobile-Responsive Design
**Requirements**:
- Touch-friendly buttons (min 44x44px)
- Collapsible sidebar for mobile
- Swipe gestures for navigation
- Bottom navigation bar for mobile
- Optimized for tablets (survey team in field)

**Expected Outcome**:
- âœ… 50% less clicks to start classification
- âœ… Intuitive for new users (no training needed)
- âœ… Modern, professional appearance
- âœ… Mobile-friendly for field teams

**Time Estimate**: 5-7 days
**Deployment**: No downtime, gradual rollout

---

### **Phase 3: Multi-Source Data Support** (Week 3-4 - Jan 20-Feb 2, 2026)
**Goal**: Support non-Kobo data sources (Excel, CSV, Google Sheets, SQL)

#### 3.1 Data Source Abstraction
**Architecture**:
```python
# app/data_sources/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py              # BaseDataSource interface
â”œâ”€â”€ kobo.py              # KoboDataSource (existing)
â”œâ”€â”€ excel.py             # ExcelDataSource (existing, refactor)
â”œâ”€â”€ csv.py               # CSVDataSource (new)
â”œâ”€â”€ google_sheets.py     # GoogleSheetsDataSource (new)
â””â”€â”€ sql.py               # SQLDataSource (new - PostgreSQL, MySQL)
```

**Interface**:
```python
class BaseDataSource:
    def read_data(self) -> pd.DataFrame:
        """Read data from source"""
        pass
    
    def get_variables(self) -> List[Variable]:
        """Detect open-ended variables"""
        pass
    
    def write_results(self, results: pd.DataFrame):
        """Write classified results"""
        pass
```

#### 3.2 Upload Interface Enhancement
**Features**:
- **Source selector**: Dropdown (Kobo / Excel / CSV / Google Sheets / SQL)
- **Source-specific options**:
  - Kobo: Asset ID, API token
  - Excel/CSV: File upload
  - Google Sheets: Sheet URL, OAuth
  - SQL: Connection string, table name
- **Smart variable detection**: Auto-detect regardless of source

#### 3.3 Output Format Options
**Formats**:
- Excel (.xlsx) - Default
- CSV (.csv) - For analytics tools
- JSON (.json) - For API integration
- SQL INSERT - Direct to database
- Google Sheets - Update existing sheet

**Expected Outcome**:
- âœ… Support all major data sources
- âœ… Flexible for different team workflows
- âœ… Future-proof architecture

**Time Estimate**: 7-10 days
**Deployment**: Backward compatible, no breaking changes

---

### **Phase 4: Tabulation Module** (Q1 2026 - Feb-Mar)
**Goal**: Auto-generate cross-tabulation tables

**Features** (see TABULATION_SPEC.md):
- Cross-tabulation with demographic variables
- Statistical significance testing
- Export to Excel with formatting
- Dashboard for tabulation history

**Integration**:
- New menu: "Tabulation" in sidebar
- Workflow: Select classified data â†’ Select demographics â†’ Generate tables
- Same Celery architecture (background processing)

**Expected Outcome**:
- âœ… Complete survey workflow (Upload â†’ Classify â†’ Tabulate â†’ Report)
- âœ… Reduce manual work by 80%

**Time Estimate**: 15-20 days
**Deployment**: Major feature release

---

## ðŸ“Š **TECHNICAL SPECIFICATIONS**

### Redis Configuration
```yaml
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
```

### Celery Configuration
```python
# celery_config.py
broker_url = 'redis://localhost:6379/0'
result_backend = 'redis://localhost:6379/1'
task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
timezone = 'Asia/Jakarta'
enable_utc = True

# Task routing
task_routes = {
    'app.tasks.classification.*': {'queue': 'classification'},
    'app.tasks.progress.*': {'queue': 'default'},
}

# Worker configuration
worker_prefetch_multiplier = 2
worker_max_tasks_per_child = 1000
task_acks_late = True
task_reject_on_worker_lost = True
```

### Gunicorn Configuration (Updated)
```python
# gunicorn.conf.py
workers = 4  # Up from 1 (multi-worker safe with Redis)
worker_class = 'sync'
worker_connections = 1000
max_requests = 2000
max_requests_jitter = 100
timeout = 120  # Reduce (requests are async now)
keepalive = 5
```

### Nginx Configuration (Updated)
```nginx
# /etc/nginx/sites-available/mcoder
upstream mcoder_app {
    server 127.0.0.1:8000;
    keepalive 32;
}

server {
    listen 80;
    server_name m-coder.flazinsight.com;
    
    client_max_body_size 100M;
    
    location / {
        proxy_pass http://mcoder_app;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Timeout configuration
        proxy_connect_timeout 600s;
        proxy_send_timeout 600s;
        proxy_read_timeout 600s;
        send_timeout 600s;
        
        # WebSocket support (for SSE)
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
    
    location /static/ {
        alias /opt/markplus/mcoder-markplus/app/static/;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }
}
```

---

## ðŸ“ˆ **PERFORMANCE TARGETS**

### Current vs Target

| Metric | Current | Target (Phase 1) | Target (Phase 4) |
|--------|---------|------------------|------------------|
| Concurrent Users | 5 (crashes at 12) | 20 | 50+ |
| Classification Speed | ~2min for 100 responses | ~1.5min | ~1min (with caching) |
| Uptime | 95% (crashes often) | 99.5% | 99.9% |
| Response Time (p95) | 3-5s | < 2s | < 1s |
| Memory Usage | ~500MB (1 worker) | ~2GB (4 workers + Redis) | ~4GB |
| CPU Usage | 80-100% (blocking) | 40-60% (async) | 30-50% |

### Scalability Testing Plan
```bash
# Load testing with Locust
# Simulate 20 concurrent users for 30 minutes
locust -f load_test.py --host=https://m-coder.flazinsight.com --users=20 --spawn-rate=2
```

**Success Criteria**:
- âœ… 0 errors under 20 concurrent users
- âœ… p95 response time < 2 seconds
- âœ… All classifications complete successfully
- âœ… No 502/504 errors
- âœ… Memory usage stable (no leaks)

---

## ðŸ—“ï¸ **IMPLEMENTATION TIMELINE**

### Week 1 (Jan 8-12, 2026) - **CRITICAL PATH**
- [x] Day 1: Plan approval, Redis installation
- [ ] Day 2: Celery setup, basic task implementation
- [ ] Day 3: Migrate classification to Celery tasks
- [ ] Day 4: Testing + fixes
- [ ] Day 5: Production deployment + monitoring

### Week 2 (Jan 13-19, 2026)
- [ ] Day 1-2: UI/UX wireframes + approval
- [ ] Day 3-4: Dashboard redesign implementation
- [ ] Day 5: Upload interface redesign
- [ ] Weekend: Testing + refinement

### Week 3 (Jan 20-26, 2026)
- [ ] Day 1-2: Results page enhancement
- [ ] Day 3: Mobile responsive optimization
- [ ] Day 4-5: User testing + fixes

### Week 4 (Jan 27-Feb 2, 2026)
- [ ] Day 1-3: Multi-source data support
- [ ] Day 4: Integration testing
- [ ] Day 5: Documentation + deployment

### Q1 2026 (Feb-Mar)
- [ ] Tabulation module development
- [ ] Advanced analytics features
- [ ] Performance optimization

---

## ðŸ’° **RESOURCE REQUIREMENTS**

### Infrastructure
- **Redis Server**: Included in VPS (2GB RAM allocated)
- **Celery Workers**: 4-8 processes (CPU intensive)
- **Total Memory**: 4GB recommended (upgrade from 2GB if needed)
- **Storage**: +10GB for Redis persistence

### Development
- **Phase 1**: 3 days full-time (critical)
- **Phase 2**: 7 days full-time (UX heavy)
- **Phase 3**: 10 days full-time
- **Phase 4**: 20 days full-time

### External
- **UI/UX Consultant**: Optional but recommended for Phase 2
- **Load Testing**: Can use free Locust
- **Monitoring**: Consider Sentry (error tracking) - $26/month

---

## ðŸš¦ **RISK MITIGATION**

### Risk 1: Downtime During Migration
**Impact**: HIGH  
**Probability**: MEDIUM  
**Mitigation**:
- Schedule maintenance window (Sunday 2-5 AM WIB)
- Prepare rollback script
- Test in development first
- Keep backup of old code

### Risk 2: Learning Curve (Celery/Redis)
**Impact**: MEDIUM  
**Probability**: LOW  
**Mitigation**:
- Extensive documentation
- Step-by-step tutorials
- Monitoring dashboards
- On-call support (developer)

### Risk 3: Performance Degradation
**Impact**: HIGH  
**Probability**: LOW  
**Mitigation**:
- Load testing before production
- Gradual rollout (A/B testing)
- Monitoring alerts (CPU, memory, error rate)
- Quick rollback capability

---

## ðŸ“ **SUCCESS METRICS**

### Phase 1 Success Criteria
- âœ… 20 concurrent users without errors
- âœ… Classification continues after browser close
- âœ… No 502 errors for 7 days
- âœ… Average uptime > 99%

### Phase 2 Success Criteria
- âœ… 50% reduction in clicks to start classification
- âœ… 90% user satisfaction score (survey)
- âœ… < 5% bounce rate on upload page
- âœ… Mobile usability score > 85 (Google PageSpeed)

### Phase 3 Success Criteria
- âœ… Support 3+ data sources
- âœ… 95% successful imports from all sources
- âœ… No regression in existing functionality

### Phase 4 Success Criteria
- âœ… Tabulation module used by 80% of users
- âœ… 80% time saving vs manual tabulation
- âœ… Positive user feedback

---

## ðŸ”§ **MONITORING & MAINTENANCE**

### Monitoring Stack
```yaml
# Recommended tools
- Application: Sentry (error tracking)
- Infrastructure: Prometheus + Grafana
- Logs: ELK Stack or Loki
- Uptime: UptimeRobot (free tier)
- APM: New Relic or DataDog (optional)
```

### Key Metrics to Track
1. **Application**:
   - Request rate (req/s)
   - Error rate (%)
   - Response time (p50, p95, p99)
   - Active users (concurrent)

2. **Celery**:
   - Task queue length
   - Task processing time
   - Worker utilization
   - Failed tasks

3. **Infrastructure**:
   - CPU usage (%)
   - Memory usage (%)
   - Disk I/O
   - Network bandwidth

4. **Business**:
   - Classifications per day
   - Success rate (%)
   - User retention
   - Average job size

### Alerting Rules
```yaml
# Alert thresholds
- Error rate > 5% for 5 minutes â†’ Critical
- Response time p95 > 3s for 10 minutes â†’ Warning
- Celery queue > 100 tasks for 15 minutes â†’ Warning
- Memory usage > 90% for 5 minutes â†’ Critical
- Failed tasks > 10 in 1 hour â†’ Warning
```

---

## ðŸ“š **DOCUMENTATION REQUIREMENTS**

### User Documentation
- [ ] User Guide (PDF + web version)
- [ ] Video tutorials (upload, classify, results)
- [ ] FAQ section
- [ ] Troubleshooting guide

### Developer Documentation
- [ ] Architecture diagram
- [ ] API documentation (if exposing API)
- [ ] Database schema
- [ ] Deployment guide
- [ ] Code comments and docstrings

### Operational Documentation
- [ ] Runbook (common issues + fixes)
- [ ] Deployment checklist
- [ ] Rollback procedure
- [ ] Monitoring guide
- [ ] Backup & restore procedure

---

## ðŸŽ“ **TEAM TRAINING**

### Technical Training (Developer)
- Redis basics (1 hour)
- Celery architecture (2 hours)
- Debugging distributed systems (1 hour)
- Performance profiling (1 hour)

### User Training (End Users)
- New UI walkthrough (30 minutes)
- Best practices (30 minutes)
- Troubleshooting (30 minutes)
- Q&A session

---

## âœ… **APPROVAL & SIGN-OFF**

**Plan Prepared By**: AI Development Team  
**Date**: January 8, 2026  
**Status**: Awaiting Approval

**Stakeholder Review**:
- [ ] Technical Lead (Developer)
- [ ] Product Owner (Haryadi)
- [ ] Infrastructure Team (VPS Admin)
- [ ] End Users Representative

**Approved By**: ________________  
**Date**: ________________

---

**Next Steps**:
1. Review and approve this plan
2. Schedule Phase 1 implementation (Week 1)
3. Allocate resources (infrastructure + time)
4. Begin execution

**Questions or Concerns**: Contact development team

---

**Document Version**: 1.0  
**Last Updated**: January 8, 2026  
**Next Review**: After Phase 1 completion
